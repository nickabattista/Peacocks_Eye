%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps
\usepackage{algpseudocode}

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
%\setlength{\headheight}{13.6pt} % Customize the height of the header
\setlength{\headheight}{2.6pt} % Customize the height of the header


\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{Scientific Computation Comp. Review} \\ [25pt] % Your university, school and/or department name(s)
\horrule{1pt} \\[0.05cm] % Thin top horizontal rule
\huge Topic: Root Finding \\ % The assignment title
\horrule{1pt} \\[0.05cm] % Thick bottom horizontal rule
}

\author{Nick Battista} % Your name

\date{\normalsize Date Created: 5/20/2014} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	Introduce the problem
%----------------------------------------------------------------------------------------

\section{Background}

The main problem we are interested in solving involves finding a root, $\alpha\in [a,b]$, such that, for some continuous function and differentiable function, $f(x)$, on $[a,b]$ that $f(\alpha)=0$. \\

Root finding can be done in n-dimensions. In most cases, going from a 1-dimensional algorithm to n-dimensions is fairly straight forward, and the analysis is pretty much analogous, but a bit hairier. We will begin by introducing a few basic algorithms and their analysis for order of convergence to root.\\

\section{The Bisection Method}

The most natural algorithm to think about, $\emph{the bisection method}$, involves successively cutting a search interval in half repeatedly until the root is found to a specified precision. However, it is clear that this is not the most efficient or elegant algorithm to find a root. (Although the author says it has many splendid applications when searching for points along arcs when functions are almost singular...mo' derivatives, mo' problems).  \\

\subsection{Bisection Algorithm}

\begin{algorithmic}

\State Let $a\leq x_{1}<x_{2} \leq b$. If $f(x_1)$ and $f(x_2)$ have different signs, i.e. $f(x_1)\cdot f(x_2)=0$. Let $\epsilon>0$, then \\

\While {$err > \epsilon$}
	\State $c = \frac{x_1+x_2}{2}$
	\If {$f(x_1)\cdot f(x_2) < 0$}
		\State $x_2 = c$
	\Else 
		\State $x_1 = c$
	\EndIf
	\State $err = x_2-x_1$
\EndWhile

\end{algorithmic}

\subsection{Analysis of Bisection / Why slow?}

$ $\\

After $k$ iterations, $x_n - x$, where $x_n = \frac{x_1+x_2}{2}$,  satisfies $\| x_n - x\| < \frac{ |b-a| } {2^{k+1}}$ \\

\emph{Why slow}? Consider we want to get 10 digit accuracy, then we need 

\begin{align} 
\nonumber
\begin{split}
& \frac{1}{2^k} < 10^{-10}\\
&k \log 2 > 10 \log 10\\
&k > 10 \frac{\log 10}{\log 2} \approx 33 \mbox{ iterations} \\
\end{split}					
\end{align}

\emph{If we know} $f$ \emph{is smooth(er), we can improve this drastically!} \\

\emph{Conclusion:} When to use the bisection method? If speed (or cuteness) doesn't matter (okay...), but if $f$ is not smooth, bisection may be the way to go. \\




%------------------------------------------------
%
% FIXED POINT ITERATIONS
%
%-------------------------------------------------

\section{Fixed Point Iterations}

$ $\\

General recursive algorithms, have a functional form such as $x_{n+1} = g(x_n, x_{n-1},...,x_{n-m})$. In the case of root finding we typically have the scenario, where $x_{n+1} = g(x_n)$. \\

If $\beta$ is a fixed point of $g(x)$ then $\beta = g(\beta)$. The way we construct $g(x)$ is such that a fixed point of $g(x)$ will also be a root of $f(x)$. That would be groovy. Our root finding problem then manifests itself as finding a fixed point for $g(x)$. Is this just a new can of worms? Maybe? Sometimes? \\

However, if we have chosen $g(x)$ in a logically sound manner, we see we should get convergence to our desired root by simple criteria- If $|g(\alpha)|<1$ in some neighborhood of $\alpha$, then convergence is guaranteed. In more mathy language, if $x_{n+1} = g(x_n)$ such that $g:\mathbb{R}\rightarrow\mathbb{R}$, $g(\alpha)=\alpha$, and $|g(\alpha)|<1$, then in some neighborhood of $\alpha$, e.g., if $|x-\alpha|<\epsilon$ for $\epsilon>0$, then $x_n\rightarrow\alpha$.\\

\subsection{Analysis of Fixed Point Iterations}

$ $\\

We will now employ the power of Taylor series to do all the heavy lifting for us. Let $\alpha$ be a root of $f(x)$, i.e., $f(\alpha)=0$ and let the error at the $(n+1)^{th}$ step we defined as, $$e_{n+1} = x_{n+1} - \alpha.$$\\
Using cute, subtle math tomfoolery, we get

\begin{align} 
\nonumber
\begin{split}
e_{n+1}&= x_{n+1} - \alpha \\
&= g(x_n) - \alpha \\
&=g(\alpha + e_n)  - \alpha \\
&=[g(\alpha) + e_n g'(\alpha) + \frac{1}{2!} e_n^2 g''(\alpha) + \frac{1}{3!} e_n^3 g'''(\alpha) + ... ] - \alpha \\
&= e_n g'(\alpha) + \frac{1}{2!} e_n^2 g''(\alpha) + \frac{1}{3!} e_n^3 g'''(\alpha) + ... , \\
\end{split}					
\end{align}

since $\alpha = g(\alpha)$ by definition of our fixed point. Hence we see that 

$$e_{n+1} = e_n g'(\alpha) + ... = O(e_n),$$

and if $g'(\alpha)<1$ we get convergence to the root. Yay. Note, however, this analysis just showed that fixed point iterations (at worst) exhibit linear convergence. Nay. \\

%------------------------------------------------
%
% NEWTON!
%
%-------------------------------------------------
\section{Newton's Method}

Probably the most widely used method, at least by Calculus 1 students for a few miserable days, is \emph{Newton's Method}. Newton's method at its roots (ha) begins by looking at a general functions Taylor Series, doing a linear approximation, and using that information to develop an algorithm to approximate a root.\\

\subsection{Derivation of Newton Method}

Consider a function $f(x)$ that is at least $C^2$, but for our purposes here we'll assume it is $C^\infty$ (We'll see why we want it to be at least $C^2$ when we do the error analysis). If we just write $f(x)$ as its Taylor series centered around some number $x_n$, we see\\

$$f(x) = f(x_n) + (x-x_n) f'(x_n) + \frac{(x-x_n)^2}{2!} f''(x_n) + \frac{(x-x_n)^3}{3!} f'''(x_n) + ...$$\\

Now do what engineers love and truncate after the linear term! \\

$$f(x) \approx f(x_n) + (x-x_n) f'(x_n)$$\\

The idea now is that if we solve for $x$ in the equation above, it will approximate a root of $f(x)$. Solving for $x$ we get \\

$$x = x_n - \frac{ f(x_n) }{ f'(x_n) }.$$\\

Finally, since we can do more than one iteration of Newton's (contrary to the dismay of Calculus students), if we let $x=x_{n+1}$ we get the usual form of Newton's, \\

$$x_{n+1} = x_n - \frac{ f(x_n) }{ f'(x_n) }.$$\\

The other way, for those geometrically minded, to derive Newton's is to pick an initial guess, say $x_n$, and to approximate the function,$f(x)$, using a tangent line at $x_n$. Finding the zero of that tangent line is what we call the next guess for the root of $f(x)$.\\

\subsection{Newton's Convergence Analysis}

Since Newton's Method is an example of a fixed point iteration, we can use the same kind of Taylor series analysis to find its convergence order. Recall for Newton's, \\

$$x_{n+1} = g(x_n) = x_n - \frac{ f(x_n) }{ f'(x_n) }.$$\\

Let $\alpha$ be a root of $f(x)$, i.e., $f(\alpha)=0$ and let the error at the $(n+1)^{th}$ step we defined as, $$e_{n+1} = x_{n+1} - \alpha.$$\\
Again doing some cute mathematical slight of hand, we get\\

\begin{align} 
\nonumber
\begin{split}
e_{n+1}&= x_{n+1} - \alpha \\
&= g(x_n) - \alpha \\
&=g(\alpha + e_n)  - \alpha \\
&=[g(\alpha) + e_n g'(\alpha) + \frac{1}{2!} e_n^2 g''(\alpha) + \frac{1}{3!} e_n^3 g'''(\alpha) + ... ] - \alpha \\
\end{split}					
\end{align}\\

Recall for fixed point schemes that $\alpha = g(\alpha)$, hence we find that \\

$$e_{n+1} = e_n g'(\alpha)  + \frac{1}{2!} e_n^2 g''(\alpha) + ... $$\\

Now we see that $g'(\alpha)=0,$ i.e.,\\

$$g'(\alpha) = \frac{d}{dx}\Bigg|_{x=\alpha} = \frac{d}{dx} \left( x - \frac{ f(x) }{ f'(x) } \right)\Bigg|_{x=\alpha} = \frac{ f(x) f''(x) }{ f'(x)^2 } \Bigg|_{x=\alpha} = \frac{ f(\alpha) f''(\alpha) }{ f(\alpha)^2 } = 0,$$\\

which shows Newton's method exhibits $2^{nd}$ order convergence to the root, i.e., $$e_{n+1} = O(e_n^2).$$ Note we see above why $f(x)$ needs to be at least $C^2$ otherwise, the statement above has no meaning. Also, we find that if $\alpha$ is not a single root that the above analysis does not imply $2^{nd}$ order convergence. \\

\subsection{What can go wrong with Newton's?}

$ $\\

Things that can go wrong:\\

\begin{itemize}
\item If a root has multiplicity greater than 1, it exhibits less than $2^{nd}$ order convergence. You can find this order by standard Taylor series analysis. {\bf{$\rightarrow$ use \emph{Aitken extrapolation} to bump up the order!}}
\item If $f(x)$ is not continuous or at least $C^2$
\item Also, sometimes computing derivatives is a pain in the butt...
\end{itemize}







%------------------------------------------------
%
% SECANT!
%
%-------------------------------------------------

\section{The Secant Method}

Now Newton has a kid sister, the \emph{Secant Method}. The essence of the Secant methods feels much of the same as Newton's; however, Secant uses the two previous iterates to compute the next approximation to the root. There are some very strong similarities.\\

Thinking geometrically, Secant's story begins with two guesses to the root. Rather than forming any tangent lines, as in Newton's case, Secant very cleverly forms a secant line between the two guesses. Analytically, we find the zero of that secant line and that root is our next guess, i.e, \\

Consider two points, $x_n$ and $x_{n-1}$, with respective function values $f(x_n)$ and $f(x_{n-12})$. The equation of the secant line between the points is

$$y(x) = \frac{ f(x_n) - f(x_{n-1}) }{ x_n - x_{n-1} } (x - x_{n}) + f(x_1).$$

Setting that line equal to zero, and solving for $x$ we find

$$x = x_n - \frac{ f(x_n) }{  \left[\frac{f(x_n) - f(x_{n-1})}{x_n - x_{n-1}}\right]  }$$

Since $x$ is our next guess we let $x_{n+1} = x$, we see

$$x_{n+1} = x_n - \frac{ f(x_n) }{  \left[\frac{f(x_n) - f(x_{n-1})}{x_n - x_{n-1}}\right]  }$$

This has a very similar form to Newton's method; however, rather than having an actual derivative in the algorithm, Secant simply employs an approximation to a derivative.











%------------------------------------------------
%
% AITKEN'S EXTRAPOLATION
%
%-------------------------------------------------

\section{Aitken Extrapolation}

The essence of Aitken is very sly, it takes a first order fixed point method and cleverly transforms it into a second order method. The only downside is for every full iteration in the Aitken scheme, we have to do the original fixed point method twice. Bummer? Nah, let's check it out! \\

Recall our original problem is finding a root, $\alpha$, of some continuous function $f(x)$. To do this, we tried to use some fixed point iterative scheme, $x_{n+1} = g(x_n)$ but only found that our scheme exhibited linear convergence. Now let's derive Aitken's scheme and make a $2^{nd}$ order method. \\

\subsection{Aitken Extrapolation Derivation}

Consider two consecutive iterations of the original fixed point scheme with their associated errors,

$$\begin{array}{cc}
x_{n+1}=g(x_n) & \ e_{n+1}=x_{n+1}-\alpha \\
x_{n+2}=g(x_{n+1}) & \ e_{n+2}=x_{n+2}-\alpha \\
\end{array}$$

Remember since the fixed point scheme only showed linear convergence we have,

$$e_{n+1} = e_n g'(\alpha) + O(e_n^2) + ... = C e_n + O(e_n^2),$$

From above we see that, \\

\begin{align} 
\begin{split}
\label{aitken1}
e_{n+1}&= x_{n+1} - \alpha = C(x_n - \alpha) \\
e_{n+2}&= x_{n+2} - \alpha = C(x_{n+1}-\alpha) \\
\end{split}					
\end{align}\\

Subtracting the top equation from the bottom, we find 

$$x_{n+2}-x_{n+1} = C(x_{n+1}-x_n),$$ and hence we get that $$C = \frac{ x_{n+2}-x_{n+1}     }{   x_{n+1}-x_n    }.$$

Now plugging that into the bottom equation in (\ref{aitken1}), we see

\begin{align} 
\begin{split}
\label{aitken1}
x_{n+2} - \alpha &= C(x_{n+1}-\alpha) \\
\alpha &= \frac{ x_{n+2} - C x_{n+1}  }{ 1 - C  } \\
\alpha &= x_{n+2} - \frac{  (x_{n+2}-x_{n+1})^2   }{ x_{n+2}-2 x_{n+1} + x_{n}  }
\end{split}					
\end{align}\\

\subsection{Algorithm: Aitken Extrapolation}

Let's discuss briefly how to implement this. It will help clarify all the elegance of this extrapolation scheme. 

\begin{algorithmic}
\State $x_0 =$ initial guess, $n=0$, $\epsilon > 0$
\While {$err > \epsilon$}
\State $y_1 = g(x_n)$
\State $y_2 = g(y_1) = g(g(x_n))$
\State $x_{n+1} = y_2 - \frac{(y_2-y_1)^2}{y_2 - 2 y_1 + x_0}$
\State $err = x_{n+1}-x_{n}$
\State $n=n+1$ 
\EndWhile
\end{algorithmic}

It is clear that for each step in Aitken's we have to do roughly $3x$ the amount of work to get a $2^{nd}$ order scheme, since we have to evaluate what are essentially 3 separate fixed point iterative scheme computations. \\

To show how Aitken's is a fixed point scheme, consider the algorithm written as:

$$x_{n+1} = H(x_n) = g(g(x_n)) - \frac{ ( g(g(x_n)) - g(x_n) )^2   }{ g(g(x_n)) - 2 g(x_n) + x_n   },$$

where $g(x)$ is the original $1^{st}$ order fixed point scheme.

\subsection{Proof Aitken Extrapolation is $2^{nd}$ Order}

Coming soon...

\end{document}