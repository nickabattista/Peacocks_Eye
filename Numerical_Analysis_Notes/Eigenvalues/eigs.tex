%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps
\usepackage{algpseudocode}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\sign}{sign}

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
%\setlength{\headheight}{13.6pt} % Customize the height of the header
\setlength{\headheight}{2.6pt} % Customize the height of the header

\newtheorem{theorem}{Theorem}
%\newtheorem{proof}{Proof}

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{Scientific Computation Comp. Review} \\ [25pt] % Your university, school and/or department name(s)
\horrule{1pt} \\[0.05cm] % Thin top horizontal rule
\huge Topic: Eigenvalues \\ % The assignment title
\horrule{1pt} \\[0.05cm] % Thick bottom horizontal rule
}

\author{Nick Battista} % Your name

\date{\normalsize Date Created: 6/25/2014 \\ Date Modified: 7/27/16} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%
%	Introduce the problem / Eigenvalue background
%
%----------------------------------------------------------------------------------------

\section{Remember eigenvalues?}

Remember how much fun calculating eigenvalues using paper and pencil was in undergraduate Linear Algebra? There was computing determinants of matrices symbollically, forming a characteristic polynomial only to find it roots, and solving some singular systems of equations. Everything that should translate swimmingly to numerically finding eigenvalues, right? Literally, none of those things are exceedingly computer friendly.\\

The process of finding eigenvalues on a computer is going to have a very different feel and approach, than how one finds them by hand. Before we dive into the abstract details, we will first flat out state that numerically computing eigenvalues tends to be a messy business. Unlike solving systems of equations (which also have a very much more natural transition from pencil and paper to the \emph{in silica} implementations), we will not be able to state \emph{a priori} the computational cost of finding eigenvalues. \\

We will not be able to say the explicit cost of finding eigenvalues; however, will only be able to approximate the number of operations required. This is all to say, \emph{numerical eigenvalue solvers need must be iterative}. We can easily observe this since at the heart of each eigenvalue problem lies a characteristic polynomial, to which we must  find its roots, and finding roots of equations is an iterative process. Whatever our method, it will require an iterative flavor, and hence will introduce errors, outside of round-off errors. \\

Before we dive right into the numerical side of things, we will first refresh a bunch of eigenvalue identities, i.e., factorizations, properties, among other things, which we will put into our numerical eigenvalue toolbox. \\


%----------------------------------------------------------------------------------------
%
%	Eigenvalue Identities, etc
%
%----------------------------------------------------------------------------------------

\section{Eigenvalue Toolbox}

This section will more or less be a list of eigenvalue identities, definitions, factorizations, and everything that is good$\ldots$in the eigen-world. Hopefully this looks familiar, if not, please indulge me, and peruse this section with a smile. \\


%
% EIGENVALUE Definitions
%
\subsection{Some eigen-related definitions to keep around for a rainy day...}

Here it goes$\ldots$let's go out and define!

\begin{enumerate}
\item $\lambda$ is an eigenvalue $\leftrightarrow$ there exists \textbf{x} such that $\lambda\textbf{x}-A\textbf{x}=0 \leftrightarrow \lambda I-A$ is \emph{singular} $\leftrightarrow \det(\lambda I - A)=0$.
%
\item \emph{Definition:} The \emph{geometric multiplicity} of an eigenvalue, $\lambda$, is the dimension of the nullspace of $\lambda I - A$, i.e., $\null(\lambda I - A)$. In short, it is the number of eigenvectors associated with one particular eigenvalue, $\lambda$.

\item \emph{Definition:} The \emph{algebraic multiplicity} of an eigenvalue, $\lambda$, of a matrix $A$, is the the multiplicity of $\lambda$ as a root of the characteristic polynomial, e.g., $p_A(\lambda) = \det(\lambda I - A) = 0$ Furthermore, note that the above two definitions imply that $$\mbox{ algebraic mult. } \geq \mbox{ geometric mult. }.$$

\item \emph{Definition:} A matrix, $A$, is said to be \emph{defective} when the algebraic multiplicity is strictly greater than the geometric multiplicity, that is, not enough linearly independent eigenvectors exist for a particular eigenvalue. \emph{Spoiler}, we will not be able to factorize $A$ into an eigenvalue decomposition, if $A$ is defective. On that note, I'll also drop here that all eigenvectors are linearly independent, so if $A$ is full rank, e.g., $\null(A)=\{ \}$, then the eigen-space of $A$ spans the same space as $A$. Stronger than that, though, we can say that the eigen-space of $A$ spans the same space as $A$.

\item \emph{Definition:} The determinant of a matrix $A$ can be written as a product of its eigenvalues, e.g., $$\det(A) = \prod_{k=1}^n \lambda_k.$$

\item \emph{Definition:} Similarly, the \emph{trace} of a matrix $A$ can be written as a sum of its eigenvalues, e.g., $$\tr(A) = \sum_{k=1}^n \lambda_k.$$ We note the above two definitions may be useful in some applications, but we will not harness them, often, if at all in everyday life. It's best to keep them around in the toolbox, though. 

\end{enumerate}


%
% EIGENVALUE FACTORIZATIONS%
\subsection{Eigenvalue Decompositions: the big guns}

If the above subsection was nails, screws, and bolts of eigenvalue problems, then this section we learn about the power tools. Matrix factorizations are very useful, as demonstrated when solving linear systems. We will restrict our discussion here to solely eigenvalue-type factorizations, e.g., eigenvalue decompositions. We will mention three different eigenvalue decompositions. \\

\begin{enumerate}

\item \emph{Diagonalization}: We can write $A$ as a product of three matrices,  $$A=X\Lambda X^{-1}.$$ if and only A is \emph{non-defective}, meaning all of $A$'s algebraic multiplicities $=$ geometric multiplicities. The matrix $X$ is composed of eigenvectors in their column form. The matrix $\Lambda$ is a diagonal matrix, where it's diagonal is composed of the eigenvalues of $A$. We note if $\Lambda_{11}=\lambda_1$, then the first column of $X$ is the eigenvector associated with $\lambda_1$, and so on. \\

You are probably familiar with this eigenvalue decomposition from undergraduate Linear Algebra and for this, I apologize. 

\item If $A\in\mathbb{R}^{n\times n}$ and has $n$ orthogonal eigenvectors, then $A$ has a \emph{unitary diagonalization}, and we can write $$A = Q\Lambda Q^{*}.$$ Similarly to matrix $X$ above, $Q$ is a matrix, whose columns are all the eigenvectors of $A$; however, in this case they are orthonormal, hence we have $QQ^{*}=Q^{*}Q=I$. Furthermore, the matrix $\Lambda$ is a diagonal matrix, which contains the eigenvalues on the diagonal. \\

We will note a few things. 

\begin{itemize}
\item This counts as both an eigenvalue decomposition and SVD, i.e., $$A=Q\Lambda Q^{*}=U\sum V^{-1},$$
where we let $U=Q$, $\sum = \Lambda$, and $V^{-1} = \sign(\Lambda)Q^{*}.$
%
\item Hermitian matrices, i.e., $A=A^{*}$, are unitary diagonalizable; e.g., all of their eigenvectors are orthogonal (orthonormal) and eigenvalues are real-valued!  Let's prove these claims!
%
\begin{proof} We will now prove that all eigenvalues are real and that their associated eigenvectors are orthogonal. 
\begin{enumerate}
%PROOF E-VALs are REAL
\item First we will prove that all the eigenvalues of a Hermitian matrix are real. To do this we will recall the infamous eigenvalue equation, $$A\textbf{x}=\lambda\textbf{x}.$$ To isolate $\lambda$ by itself we multiply both sides by $\textbf{x}^{*}$, and solve for $\lambda$, e.g., $$\lambda = \frac{ \textbf{x}^{*} A \textbf{x}    }{  \textbf{x}^{*} \textbf{x}   }.$$
Now we are ready for the final step, ,
$$\lambda^{*} = \left( \frac{ \textbf{x}^{*} A \textbf{x}    }{  \textbf{x}^{*} \textbf{x}   } \right)^{*}  = \frac{ \textbf{x}^{*} A^{*} \textbf{x}    }{  \textbf{x}^{*} \textbf{x}   } =  \frac{ \textbf{x}^{*} A \textbf{x}    }{  \textbf{x}^{*} \textbf{x}   } = \lambda,$$ and hence $\lambda\in\mathbb{R}.$

%PROOF E-VECs are Orthogonal
\item We now will prove all the eigenvectors are orthogonal. Assume that$\lambda$ and $\mu$ are distinct eigenvalues associated eigenvectors $\textbf{x}$ and $\textbf{y}$, respectively. Hence we will now show that $<\textbf{x},\textbf{y}>=0$

To begin we consider, $$\lambda<\textbf{x},\textbf{y}> = <\lambda\textbf{x},\textbf{y}> = <A\textbf{x},\textbf{y}>.$$

Now using properties of inner products, we can write $$<A\textbf{x},\textbf{y}> = <\textbf{x},A^{*}\textbf{y}>.$$

However, since $A=A^{*}$, we get

$$<\textbf{x},A^{*}\textbf{y}> = <\textbf{x},A\textbf{y} > = <\textbf{x},\mu\textbf{y}>$$

Using another property of inner products, we can pull out a scalar by 
$$<\textbf{x},\mu\textbf{y}> = \mu^{*}<\textbf{x},\textbf{y}> = \mu <\textbf{x},\textbf{y}>.$$

Hence we have $$ \lambda <\textbf{x},\textbf{y}> =  \mu <\textbf{x},\textbf{y}>,$$ but since we assumed $\mu\neq\lambda$, this implies that $<\textbf{x},\textbf{y}>=0.$\\ 

I have to admit this is a more long winded way to show this property of Hermitian matrices. Since we are assuming $A$ is full rank, we can diagonalize $A$, e.g., let $$A=X\Lambda X^{-1},$$

and now since $A=A^{*}$, we have $$A^{*} = ( X\Lambda X^{-1})^{*} = (X^{-1})^{*} \Lambda^{*} X^{*} = ( X\Lambda X^{-1})^{*} = (X^{-1})^{*} \Lambda X^{*},$$

since all the eigenvalues are real. However this must be equal to the original diagonalization of $A$ because $A$ is Hermitian. Therefore we have that $$X=(X^{-1})^{*} \ \ \ \mbox{ and } \ \ \ X^{-1} = X^{*}.$$

The only way this above statement could be true is if $X^{*}=X^{-1}$, which makes $X$ an orthogonal matrix, and our result falls out. 

\end{enumerate}
\end{proof}

In fact, we can extend this to all matrices that are normal! \\
\begin{theorem} A matrix is unitary diagonalizable if and only if it is normal. \end{theorem}  Recall that a matrix, $A$, is \emph{normal} if $AA^{*} = A^{*}A$.
\end{itemize}


%Schur Factorization
\item Regardless of the spectral properties of $A$, we can write it in its \emph{Schur form}. The Schur form of a matrix is where A is decomposed as, $$A=QTQ^{*}.$$ where $Q$ is an orthogonal matrix and $T$ is an upper-triangular matrix. Please note that $Q$ is not, in general, composed of eigenvectors, as in the unitary diagonalization case. Moreover, upper-triangular matrix $T$ is in the \emph{Jordan form}, but still maintains all the eigenvalues of $A$ along its main diagonal. This is because $A$ and $T$ are \emph{similar}. We now define what that means.

\begin{itemize}
\item \emph{Definition}: Two matrices, $A$ and $B$, are similar if and only if there exists non-singular matrix $P$, such that we can write $A=PBP^{-1}.$ Furthermore if two matrices are similar, then they have the same eigenvalues. A clear example of this is the diagonalization of a matrix. 
\end{itemize}

Moreover, because of the \emph{Jordan form} of $T$, every matrix has a Schur Factorization. That's important enough to write as a theorem, give me a second. 

\begin{theorem} Every square matrix has a Schur Factorization.
\end{theorem}

Boom. Why is this so important? Well as we approach numerically finding eigenvalues, we will almost exclusively use the Schur Factorization. The only time we won't is typically when $A$ is Hermitian, and we see the Schur Factorization becomes a Unitary Diagonalization of  $A$, that is, $T$ is diagonal. Also, whenever we have orthogonal matrices involved, computations tend to have nice stability properties, from the backward stability of how one will compute $Q$, i.e., using Householder Reflectors or Givens Rotations. 

\end{enumerate}

Before jumping head first into a bunch of algorithms, we will give a brief overview of where every algorithm fits into the mix. Please do not be intimidated by the names, but simply sit back and welcome some new key words and connections into your brain.



%----------------------------------------------------------------------------------------
%
%	OVERVIEW OF NUMERICAL EIGENVALUE METHODS
%
%----------------------------------------------------------------------------------------

\section{The Overview: the long and the short of it}

This section's goal is not to confuse or intimidate you, if that happens, I am all apologies, but to simply put all the methods into a context and where they fit into the world of standard eigenvalue schemes. We will not go into detail about each method here, but will just say one or two useful notes that differentiate them from each other. Here comes the list!

\begin{enumerate}

% RAYLEIGH QUOTIENT
\item The \emph{Rayleigh Quotient}, or $RQ$, can be used to find an exact eigenvalue, if the true eigenvector is known. Furthermore, if an approximation to an eigenvector is known, then we can use the $RQ$ to find the eigenvalue that would be associated with it, that is, we can find an approximate eigenvalue if we have an approximate eigenvector. s

% POWER ITERATIONS / INV. POWER ITERATION
\item There are two methods that can only find one eigenvector - the \emph{Power Iteration} and \emph{Inverse Power Iteration}.
\begin{enumerate}
\item The \emph{Power Method} only finds the eigenvector associated with the largest eigenvalue in magnitude. That is the only eigenvector it can find. 
\item The \emph{Inverse Power Method} only, too, finds one eigenvector at a time; however, it finds the eigenvector associated with eigenvalue that is nearest to the shift, $\mu$, that is an input to the algorithm. In short, we have \emph{a priori} information that $\mu$ is close to an eigenvalue, so using $\mu$ we are able to find the eigenvector associated with the eigenvalue closest to $\mu$. However, we have find the whole spectrum of a matrix using this method$\ldots$with enough patience and persistence. 
\item Once we have an approximate eigenvector, coming from either the Power Method or Inverse Power Iteration, we employ  the $RQ$ to find the associated approximate eigenvalue.
\end{enumerate}

% QR Algorithm
\item The \emph{QR Algorithm} is able to find all the eigenvalues of a matrix at once. However, it can be both computationally expensive and slow, if not performed optimally. There are two routes one can use when using the \emph{QR Algorithm}, which we will briefly put into context here. 
\begin{itemize}
\item[\textbf{Route 1}]: Apply $QR$ algorithm directly to the matrix $A\in\mathbb{R}^{N\times N}$. This is \emph{slow} and \emph{expensive}. Basically the overall cost will be computing the $QR$ factorization of a matrix at each iteration, which costs $\mathcal{O}(N^3)$, so if the algorithm iterates $m$-times, the cost be will $\sim \frac{4}{3}mN^3$. This may not seem like a big deal, but $m$ could potentially be rather large, making the potential overall cost for the algorithm $\mathcal{O}(N^4).$
\item[\textbf{Route 2}]: Before applying the $QR$ algorithm right away to $A$, we will massage it into upper-Hessenberg form, $H$, and then apply the $QR$ algorithm in two steps, e.g., 
\begin{itemize}
\item[\textbf{Step 1}]: Massage the matrix to upper Hessenberg form, $H$, using Householder Reflectors or Givens Rotations, which costs $\mathcal{O}(N^3)$. Note, if $A$ is exceptionally large, one can use a Krylov method, i.e., the \emph{Arnoldi} iteration, or \emph{Lanczos} iteration, if the matrix is Hermitian, to transform the matrix to upper Heisenberg form. Furthermore, this step is backward stable! 
\item[\textbf{Step 2}]: Apply the $QR$ algorithm to the upper Heisenberg matrix, $H$. Each step will now cost $\mathcal{O}(N^2)$, making these two steps computationally less expensive and faster than simply applying the $QR$ algorithm to the original matrix $A$. 
\end{itemize}
\end{itemize}

\end{enumerate}

The long and the short of it is, if you only care about one particular eigenvalue, use the Power Iteration or Inverse Power Iteration, but if you need the full spectrum of $A$, use the $QR$ algorithm and use \emph{Route 2}, that is the two step eigenvalue solving scheme. \\

We will now proceed with the first algorithms anyone learns when computing eigenvalues and eigenvectors numerically - \emph{Rayleigh Quotient}, \emph{Power Iteration}, and \emph{Inverse Power Iteration}.


%----------------------------------------------------------------------------------------
%
%	Rayleigh Quotient, Power Iteration, and Inverse Power Iteration
%
%----------------------------------------------------------------------------------------

\section{Rayleigh Quotient, Power Iteration, and Inverse Power Iteration - oh my!}

Let's begin the algorithmic discussion by analyzing the methods that can only give you one eigenvector, or eigenvalue, either at a time, or in total. We will perform error analysis, as well, so get ready for some fun! Our journey begins with the Rayleigh Quotient.

%
% RAYLEIGH QUOTIENT
%
\subsection{Rayleigh Quotient}

The Rayleigh Quotient pops right out of the eigenvalue equation, $A\textbf{x} = \lambda \textbf{x}$, and solving for $\lambda$, i.e., 
\begin{align*}
\lambda \textbf{x} &= A\textbf{x} \\ 
\lambda (\textbf{x}^{*} \textbf{x}) &= \textbf{x}^{*} A\textbf{x} \\ 
\lambda &= \frac{  \textbf{x}^{*} A\textbf{x}       }{  \textbf{x}^{*}\textbf{x}       }. \\
\end{align*}

The last line is what we define the Rayleigh Quotient to be, 
\begin{equation}
\label{rayleigh_quotient} r(\textbf{x}) =  \frac{  \textbf{x}^{*} A\textbf{x}       }{  \textbf{x}^{*}\textbf{x}  }.
\end{equation}

However, like mentioned before, the $RQ$ is not only useful for computing the exact eigenvalue when one knows the exact eigenvector. Furthermore, the $RQ$ minimizes the $2$-norm of $||A\textbf{x}-\alpha\textbf{x}||_2.$ To do this, we begin by minimizing $r(x)$, e.g.,
\begin{align*}
\frac{ \partial r(x) }{ \partial x_j } &= \frac{  \frac{ \partial}{ \partial x_j} \big( \textbf{x}^* A\textbf{x} \big) }{  \textbf{x}^*\textbf{x}  } - \frac{ \big(\textbf{x}^*A\textbf{x}  \big) \frac{\partial}{\partial x_j }  (\textbf{x}^*\textbf{x} )    }{  \textbf{x}^* \textbf{x} } \\ \\
%
	&= \frac{ 2(A\textbf{x})_j }{   \textbf{x}^*\textbf{x} } - \frac{  \big(\textbf{x}^*A\textbf{x}\big)(2x_j)    }{ ( \textbf{x}^*\textbf{x})^2      } \\ \\ 
	&= \frac{2}{ \textbf{x}^*\textbf{x} } \left[  (A\textbf{x})_j - \left( \frac{  \textbf{x}^*A\textbf{x} }{ \textbf{x}^*\textbf{x} }      \right) x_j       \right] \\ \\
	&= \frac{2}{ \textbf{x}^*\textbf{x} } \bigg(  A\textbf{x} - r(\textbf{x}) \textbf{x}   \bigg)_j \\
\end{align*}

and hence we have that $$\nabla r(\textbf{x}) = \frac{2}{ \textbf{x}^*\textbf{x} } \big[  A\textbf{x} - r(\textbf{x}) \textbf{x} \big].$$

Therefore $r(x)$ is minimized when \textbf{x} is an eigenvector and $r(\textbf{x})$ is its associated eigenvalue. However, if $\textbf{x}$ is not a true eigenvector of $A$, but an approximation to one, how accurate will the $RQ$ be? It turns out that if \textbf{x} is sufficiently close to a true eigenvector, then the $RQ$ is quadratically accurate. We will go ahead and prove this now!

%
% PROOF RQ is QUADRATICALLY ACCURATE
%
\subsubsection{Proof $RQ$ is Quadratically Accurate}

\begin{proof}

To begin this proof, we will assume that $\{ \textbf{v}_j \}$ are the true eigenvectors and $\textbf{x}$ is our approximated eigenvector. We can expand \textbf{x} in terms of the eigenvector basis, $$\textbf{x} = \sum_j a_j \textbf{v}_j.$$

Now we begin. Where we are going, we don't need roads, I mean we will assume that if \textbf{x} is sufficiently close to the eigenvector $\textbf{v}_P$, then we have $\left| \frac{a_k}{a_P}  \right| < \epsilon$ for all $k\neq P$. Before we use this fact, we must first do some computations. Let's begin with $RQ$,

\begin{align*}
r(\textbf{x} )&= \frac{  \sum_j a_j \textbf{v}_j^{*} A  \sum_k a_k \textbf{v}_k   }{  \sum_j a_j \textbf{v}_j^{*} \sum_k a_k \textbf{v}_k   } \\ \\
	%
	&= \frac{ \sum_j a_j \textbf{v}_j^{*} \sum_k a_k (A \textbf{v}_k)   }{  \sum_j \sum_k a_j a_k \textbf{v}_j^{*} \textbf{v}_k   } \\ \\
	%
	&=  \frac{ \sum_j a_j \textbf{v}_j^{*} \sum_k a_k ( \lambda_k \textbf{v}_k )  }{  \sum_j \sum_k a_j a_k \textbf{v}_j^{*} \textbf{v}_k   } \\ \\
	%
	&= \frac{   \sum_j \sum_k a_j a_k \lambda_k \textbf{v}_j^{*} \textbf{v}_k     }{   \sum_j \sum_k a_j a_k \textbf{v}_j^{*} \textbf{v}_k        } \\ 
\end{align*}

Now we proceed to subtract $\lambda_P$ from $r(\textbf{x})$, e.g., 

\begin{align*}
r(\textbf{x}) - \lambda_P &= \frac{   \sum_j \sum_k a_j a_k \lambda_k \textbf{v}_j^{*} \textbf{v}_k     }{   \sum_j \sum_k a_j a_k \textbf{v}_j^{*} \textbf{v}_k        } - \lambda_P \\ \\
	%
	&= \frac{ \sum_{j\neq P} \sum_{k\neq P} \left( \frac{a_ja_k}{a_P^2} \right) \lambda_k \textbf{v}_j^{*}\textbf{v}_k + \lambda_P     }{  \sum_{j\neq P} \sum_{k\neq P} \left( \frac{a_ja_k}{a_P^2} \right) \textbf{v}_j^{*}\textbf{v}_k + 1  } - \lambda_P \\ 
\end{align*}

Using one of our favorite Taylor Series, $\frac{1}{1+x} = 1-x+x^2-x^3+\ldots$, since $\left| \frac{a_k}{a_P}  \right| < \epsilon$, we get

\begin{align*}
	&= \Big( \sum_{j\neq P} \sum_{k\neq P} \left( \frac{a_ja_k}{a_P^2} \right) \lambda_k \textbf{v}_j^{*}\textbf{v}_k + \lambda_P   \Big) \left[  1 - \left( \sum_{j\neq P} \sum_{k\neq P} \left( \frac{a_ja_k}{a_P^2} \right) \textbf{v}_j^{*}\textbf{v}_k   \right) +  \ldots  \right] - \lambda_P \\ \\
	%
	&= \left[  \sum_{j\neq P} \sum_{k\neq P} \left( \frac{a_ja_k}{a_P^2} \right) \lambda_k \textbf{v}_j^{*}\textbf{v}_k + \lambda_P - \lambda_P \sum_{j\neq P} \sum_{k\neq P} \left( \frac{a_ja_k}{a_P^2} \right)  \textbf{v}_j^{*}\textbf{v}_k + \lambda_P    \right] + \mbox{ \emph{higher order terms} } \\ \\
	%
	&\approx \mathcal{O}(\epsilon^2). \\
\end{align*}

\end{proof}

Hence we find that $r(\textbf{x}) - \lambda_P = \mathcal{O}(\epsilon^2).$ Welp, now that we know how accurate $RQ$ is, let's dive into some methods that produce approximate eigenvectors so we can put the $RQ$ to use! We will introduce the Power Method, followed closely behind by it's cousin, the Inverse Power Iteration. 



%
% POWER ITERATION
%
\subsection{Power Iteration}

The Power Iteration, a method appropriately named as we will be hammering an initial guess of an eigenvector by the matrix $A$, over and over. That is, we will just continually multiply $A$ onto the vector, $\textbf{x}^{(0)}$. It turns out that by continually multiply $A$ onto $\textbf{x}^{(0)}$,  it will begin to align itself in the direction of the eigenvector associated with the largest eigenvalue in magnitude. \\

The algorithm can be written simply as, 

\begin{align*}
\textbf{x}^{(0)} &\mbox{ s.t. } \left|\left|v^{(0)}\right|\right| = 1 \\ 
\mbox{for } &k=1,2,\ldots \\
\ \ \ \ &\ \textbf{w} = A\textbf{x}^{(k-1)} \\
\ \ \ \ &\ \textbf{x}^{(k)} = \frac{ \textbf{w} } { \left|\left| \textbf{w} \right|\right| } \\
\ \ \ \ &\lambda^{(k)} = \bigg( \textbf{x}^{(k)} \bigg)^{*} A \textbf{x}^{(k)} \\
\end{align*}

Well, how does this actually work? It turns out the analysis is not terribly complicated. Let's show why this converges to the eigenvector associated with the largest eigenvalue in magnitude. First we expand the initial guess vector in terms of the eigenbasis,

$$\textbf{x}^{(0)} = a_1 \textbf{v}_1 + a_2 \textbf{v}_2 + \ldots + a_N \textbf{v}_N.$$

Next after iterating $k$ times, i.e., $k-$multiples of $A$ onto $\textbf{x}^{(0)}$ and letting $c_k$ be a normalization parameter and assuming that $\left| \lambda_1 \right| > \left| \lambda_2 \right| >\ldots> \left| \lambda_N \right|$, we get

\begin{align*}
\textbf{x}^{(k)} &= c_k A^k \textbf{x}^{(0)} \\
	&= c_k \Big( a_1 A^k \textbf{v}_1 + a_2 A^k \textbf{v}_2 +\ldots +  a_N A^k \textbf{v}_N  \Big) \\
	&= c_k \Big( a_1 \lambda_1^k \textbf{v}_1 + a_2 \lambda_2^k \textbf{v}_2  +\ldots+ a_N \lambda_N^k \textbf{v}_N   \Big)\\
	&= c_k \lambda_1^k \left(  a_1 \textbf{v}_1 + a_2\left( \frac{\lambda_2}{\lambda_1}  \right)^k \textbf{v}_2 +\ldots+  a_N\left( \frac{\lambda_N}{\lambda_1}  \right)^k \textbf{v}_N    \right). \\
\end{align*}

As $k\rightarrow\infty$, we see this process will converge to $\textbf{x}_{\infty} = c_{\infty} \lambda^{\infty} a_1 \textbf{v}_1$. If you forgive that abusive notation, it is clear that the Power Method will be able to capture the single eigenvector associated with the largest eigenvalue in magnitude. Unfortunately, this process will not be able to produce any others for us, but that is where \emph{Inverse Power Iteration} comes in. \\

Before we get to that algorithm, we will discuss the convergence of the Power Method to the eigenvector as well as the convergence to its associated eigenvalue, using $RQ$.

%
% Convergence of PM to Dominant E-Vec
%
\subsubsection{Convergence of Power Method to Dominant Eigenvector}

 Here we will show that the convergence of the Power Method to the dominant eigenvector is $$\left|\left| \textbf{x}^{(k)} - (\pm \textbf{v}_1) \right| \right| = \mathcal{O}\left( \left| \frac{\lambda_2}{\lambda_1}  \right|^k  \right).$$
 
 \begin{proof}
 
We will begin with $\textbf{x}^{(k)} - (\pm \textbf{v}_1)$ and massage it until we get our result. Let's take a look.
\begin{align*}
\textbf{x}^{(k)} - (\pm \textbf{v}_1) &= c_k \left( \sum_{j=1}^N a_j \lambda_j^k \textbf{v}_j \right) - (\pm \textbf{v}_1) \\ \\
	%
 	&= c_k \lambda_1^k a_1 \textbf{v}_1 + c_k \sum_{j=2}^N a_j \left( \frac{ \lambda_j}{\lambda_1} \right)^k \textbf{v}_j - (\pm \textbf{v}_1). \\
\end{align*}

Letting the normalization factor, be $c_k = \frac{1}{ \lambda^k a_1 }$, we get

$$\textbf{x}^{(k)} - (\pm \textbf{v}_1) = c_k \sum_{j=2}^N a_j \left( \frac{ \lambda_j}{\lambda_1} \right)^k \textbf{v}_j,$$ 

and the largest error term will be from the $j=2$ term, and hence we obtain the final result, 

$$\left|\left| \textbf{x}^{(k)} - (\pm \textbf{v}_1) \right| \right| = \mathcal{O}\left( \left| \frac{\lambda_2}{\lambda_1}  \right|^k  \right).$$

\end{proof}

Welp, we got the result, although the convergence is only linear, but hey, it still converges! Not too bad for such a simple algorithm. What does this mean for the convergence its associated eigenvalue? 

%
% Convergence of PM for Associated Eigenvalue
%
\subsubsection{Convergence of Power Method for Dominant Eigenvalue}

We will show that using the Power Method for finding the dominant eigenvalue converges as, $$\left|\left| r(\textbf{x}^{(k)} - \lambda_1 \right|\right| = \mathcal{O}\left( \epsilon^{2k} \right).$$

\begin{proof}

This is a rather simple proof, in which we will just simply use our previous results for the accuracy of the $RQ$ and the convergence of the Power Method to the dominant eigenvector. First from the accuracy of the $RQ$ we get 

$$r(\textbf{x}^{(k)} - r(\textbf{v}_1) =  \mathcal{O}\left(\epsilon^k\right),$$

and hence $$r(\textbf{x}^{(k)} - r(\textbf{v}_1) = \mathcal{O}\left( \left|\left| \textbf{x}^{(k)} - \textbf{v}_1   \right|\right|^2  \right).$$

Now using the convergence of the Power Method to dominant eigenvector, we find that $$r(\textbf{x}^{(k)} - r(\textbf{v}_1) = \mathcal{O}\left( \left|\left| \textbf{x}^{(k)} - \textbf{v}_1   \right|\right|^2  \right) = \mathcal{O}\left( \left( \epsilon^k \right) ^2 \right) = \mathcal{O}\left( \epsilon^{2k}  \right).$$

\end{proof}

Now that we have beat the Power Method to death, and it can only give us the dominant eigenvector approximation, let's set our sights a bit higher$\ldots$finding any eigenvector we want, by guessing different eigenvalue approximations. Yup, it's time for the \emph{Inverse Power Iteration}!


%------------------------------------------------------------------------
%
% INVERSE POWER ITERATION
%
%------------------------------------------------------------------------

\subsection{Inverse Power Iteration}

The idea with \emph{Inverse Power Iteration}, $IPI$, is that if we have background knowledge of what the spectrum of $A$ looks like, we will be able to use that information to pull out what the exact eigenvalues, well eigenvectors, are. We will do this by introducing shifts, $\mu$, into the Power Method. However, rather than apply perform the Power Method by continually multiplying $A$ onto an initial guess, we will continually multiply the matrix $(A-\mu I)^{-1}$.\\

The reason for this comes from relations between this shifted matrix, $(A-\mu I)^{-1}$ and $A$. We note two things, 
\begin{enumerate}
\item The eigenvectors of $(A-\mu I)^{-1}$  are the same as $A$'s except they have eigenvalues $\left\{ \frac{1}{\lambda_j-\mu } \right\}$, rather than $\{ \lambda_j \}$.
\item If $\mu$ is close to $\lambda_j$, then $\frac{1}{\lambda_j-\mu} >> \frac{1}{\lambda_k -\mu}$ for $k\neq j$.
\end{enumerate}

We can easily see the first of the above statements is true, i.e., 
\begin{proof}
We begin with the eigenvalue equation and go from there,
$$A\textbf{v} = \lambda \textbf{v}.$$
Now subtract $\mu I$ from both sides, 
$$(A-\mu I) \textbf{v} = (\lambda-\mu) \textbf{v}.$$
Finally we just need to multiply both sides by $(A-\mu I)^{-1}$ and divide both sides by $(\lambda-\mu)$, to get
$$(A-\mu I)^{-1} \textbf{v} = \left( \frac{1}{\lambda-\mu} \right) \textbf{v}.$$
Hence we see that $A$ and $(A-\mu I)^{-1}$ have the same eigenvectors, and the eigenvalues of $(A-\mu I)^{-1}$ are $\left\{ \frac{1}{\lambda_j-\mu } \right\}$.
\end{proof}

Similar to the Power Method, the convergence of the $IPI$ is only linear. Furthermore, the algorithm looks awfully similar to that of the Power Method; however, each step is going to require solving a linear system, now

\begin{align*}
\textbf{x}^{(0)} &\mbox{ s.t. } \left|\left|v^{(0)}\right|\right| = 1 \\ 
\mbox{for } &k=1,2,\ldots \\
\ \ \ \ &\ \mbox{\emph{Solve}}: \bigg(A-\mu I\bigg)\textbf{w} = \textbf{x}^{(k-1)} \\
\ \ \ \ &\ \textbf{x}^{(k)} = \frac{ \textbf{w} } { \left|\left| \textbf{w} \right|\right| } \\
\ \ \ \ &\lambda^{(k)} = \bigg( \textbf{x}^{(k)} \bigg)^{*} A \textbf{x}^{(k)} \\
\end{align*}

One question you may be asking yourself is, if we're computing $\lambda^{(k)}$ each iteration with the $RQ$, why aren't we using that information anywhere? Also, does the shift, $\mu$, need to stay the same as the computation progresses? The answer is, lets use the extra information to update our shift each iteration! This should cause the algorithm to converge faster, too (and it does!)!\\

This process is now called the \emph{Rayleigh Quotient Iteration}!

\subsubsection{Rayleigh Quotient Iteration}

The \emph{Rayleigh Quotient Iteration} is virtually the same as the Inverse Power Iteration, except each iteration with update the shift with $\mu^{(k)} = \lambda^{(k)}.$ The algorithm then becomes

\begin{align*}
\textbf{x}^{(0)} &\mbox{ s.t. } \left|\left|v^{(0)}\right|\right| = 1 \\ 
\lambda^{(0)} &= \left( \textbf{v}^{(0)} \right)^{*}A  \textbf{v}^{(0)} \\
\mbox{for } &k=1,2,\ldots \\
\ \ \ \ &\ \mbox{\emph{Solve}}: \bigg(A-\lambda^{(k-1)} I\bigg)\textbf{w} = \textbf{x}^{(k-1)} \\
\ \ \ \ &\ \textbf{x}^{(k)} = \frac{ \textbf{w} } { \left|\left| \textbf{w} \right|\right| } \\
\ \ \ \ &\lambda^{(k)} = \bigg( \textbf{x}^{(k)} \bigg)^{*} A \textbf{x}^{(k)} \\
\end{align*}

We get the following convergence properties,
\begin{align*}
\left|\left| \textbf{x}^{(k+1)} - \left( \pm \textbf{v}_P \right) \right|\right| &= \mathcal{O}\left( \left|\left| \textbf{x}^{(k)} - \left( \pm \textbf{v}_P \right)   \right|\right|^3   \right) \\ \\
\left| \lambda^{(k+1)} - \lambda_P \right| &= \mathcal{O}\left( \left| \lambda^{(k)} - \lambda_P  \right|^3   \right) \\
\end{align*}

that is, we have cubic convergence! These proofs are very similar to those from the Power Method. The sketch of the proof is the following. If we assume $\textbf{x}^{(k)}$ is sufficiently close to the true eigenvector, $\textbf{v}_P$, then we have
 $$\left|\left|  \textbf{x}^{(k)} - \left( \pm \textbf{v}_P \right)   \right|\right| \leq \epsilon,$$
and also from $RQ$ that 
$$\left| \lambda^{(k)} - \lambda_P  \right| = \mathcal{O}\left( \epsilon^2 \right).$$
Now if we go ahead and do one more iteration of the Rayleigh Quotient Iteration we get,
$$ \left|\left| \textbf{x}^{(k+1)} - \left( \pm \textbf{v}_P \right) \right|\right| = \mathcal{O}\Bigg(  \left| \lambda^{(k)} - \lambda_P  \right|       \left|\left| \textbf{x}^{(k)} - \left( \pm \textbf{v}_P \right) \right|\right|    \Bigg) = \mathcal{O}\left( \epsilon^3  \right).$$

One last thing to note is the computational expense of the Rayleigh Quotient Iteration (or Inverse Power Iteration for that matter). Due to solving a linear system at each iteration, this could become very expensive, rather quickly$\ldots$making Rayleigh Quotient Iteration definitely the ideal choice over the Inverse Power Iteration, since it exhibits cubic convergence. The expense is as follows,
\begin{enumerate}
\item If $A$ is a dense matrix, then solving $\bigg(A-\lambda^{(k-1)} I\bigg)\textbf{w} = \textbf{x}^{(k-1)}$ will most likely take $\mathcal{O}\left(N^3 \right)$ operations.
\item If $A$ is upper-Hessenberg, then each step would take $\mathcal{O}\left( N^2  \right)$ operations.
\item If $A$ is tridiagonal, then each step takes $\mathcal{O}(N)$ operations. 
\end{enumerate}


\end{document} %%%ENDS DOCUMENT 
